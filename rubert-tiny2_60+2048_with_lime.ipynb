{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7yNx3-x8ZdAP"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scibox import switch_to_scibox, switch_to_huggingface\n",
    "switch_to_huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mZUKZSW7SuKr",
    "outputId": "310ca31f-6c0a-4a41-8409-fc6f32f9c623"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.0a0+81ea7a4)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.39.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -U accelerate\n",
    "! pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GqtaKhzhZxTj"
   },
   "outputs": [],
   "source": [
    "# –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "#df = pd.read_excel('clean_dataset_3_credit_ratings_agencies_v2.0.xlsx')\n",
    "df = pd.read_excel('clean_train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>pr_txt</th>\n",
       "      <th>–ö–∞—Ç–µ–≥–æ—Ä–∏—è</th>\n",
       "      <th>–£—Ä–æ–≤–µ–Ω—å —Ä–µ–π—Ç–∏–Ω–≥–∞</th>\n",
       "      <th>pr_txt_cleaned</th>\n",
       "      <th>sentences_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–æ–º–ø...</td>\n",
       "      <td>BB</td>\n",
       "      <td>BB</td>\n",
       "      <td>¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–æ–º–ø...</td>\n",
       "      <td>–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–æ–º–ø–∞–Ω–∏—è —É—Ä–æ–≤–µ–Ω—å...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø—Ä–∏—Å–≤–æ–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –ü–ê–û ¬´–§...</td>\n",
       "      <td>AAA</td>\n",
       "      <td>AAA</td>\n",
       "      <td>¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø—Ä–∏—Å–≤–æ–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –ü–ê–û ¬´–§...</td>\n",
       "      <td>–ø—Ä–∏—Å–≤–æ–∏—Ç—å –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ —É—Ä–æ–≤–µ–Ω—å –º–∞—Ä—Ç —Ä–µ–π—Ç–∏...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–≤—ã—Å–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –û–ê–û ¬´–ú–†...</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA+</td>\n",
       "      <td>¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–≤—ã—Å–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –û–ê–û ¬´–ú–†...</td>\n",
       "      <td>–ø–æ–≤—ã—Å–∏—Ç—å –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ —É—Ä–æ–≤–µ–Ω—å –º–∞—Ä—Ç —Ä–µ–π—Ç–∏–Ω...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–Ω–∏–∑–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –ü–ê–û ¬´–ú....</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–Ω–∏–∑–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –ü–ê–û ¬´–ú....</td>\n",
       "      <td>–ø–æ–Ω–∏–∑–∏—Ç—å –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ —É—Ä–æ–≤–µ–Ω—å –∏–∑–º–µ–Ω–∏—Ç—å –ø—Ä...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–≤—ã—Å–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–æ–º–ø–∞–Ω–∏...</td>\n",
       "      <td>BB</td>\n",
       "      <td>BB+</td>\n",
       "      <td>¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–≤—ã—Å–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–æ–º–ø–∞–Ω–∏...</td>\n",
       "      <td>–ø–æ–≤—ã—Å–∏—Ç—å –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–æ–º–ø–∞–Ω–∏—è —É—Ä–æ–≤–µ–Ω—å –∏–∑...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>880</td>\n",
       "      <td>–ù–ö–† –ø—Ä–∏—Å–≤–æ–∏–ª–æ –≥—Ä—É–ø–ø–µ \"–†–ö–° –î–µ–≤–µ–ª–æ–ø–º–µ–Ω—Ç\" –∫—Ä–µ–¥–∏—Ç–Ω...</td>\n",
       "      <td>BBB</td>\n",
       "      <td>BBB-</td>\n",
       "      <td>–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –§–∞–∫—Ç–æ—Ä—ã, –æ–ø—Ä...</td>\n",
       "      <td>–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã–π –¥–µ–π—Å—Ç–≤–∏–µ —Ñ–∞–∫—Ç–æ—Ä –æ–ø—Ä–µ–¥–µ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>908</td>\n",
       "      <td>–ù–ö–† –ø—Ä–∏—Å–≤–æ–∏–ª–æ –∑–∞–≤–æ–¥—É –ö—Ä–∏–∞–ª–≠–Ω–µ—Ä–≥–æ–°—Ç—Ä–æ–π –∫—Ä–µ–¥–∏—Ç–Ω—ã...</td>\n",
       "      <td>BBB</td>\n",
       "      <td>BBB-</td>\n",
       "      <td>–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –§–∞–∫—Ç–æ—Ä—ã, –æ–ø—Ä...</td>\n",
       "      <td>–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã–π –¥–µ–π—Å—Ç–≤–∏–µ —Ñ–∞–∫—Ç–æ—Ä –æ–ø—Ä–µ–¥–µ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>978</td>\n",
       "      <td>–ù–ö–† –ø—Ä–∏—Å–≤–æ–∏–ª–æ –≥—Ä—É–ø–ø–µ ¬´–°–∞–º–æ–ª–µ—Ç¬ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏...</td>\n",
       "      <td>A</td>\n",
       "      <td>A-</td>\n",
       "      <td>–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –§–∞–∫—Ç–æ—Ä—ã, –æ–ø—Ä...</td>\n",
       "      <td>–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã–π –¥–µ–π—Å—Ç–≤–∏–µ —Ñ–∞–∫—Ç–æ—Ä –æ–ø—Ä–µ–¥–µ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>1016</td>\n",
       "      <td>–ù–ö–† –ø—Ä–∏—Å–≤–æ–∏–ª–æ ¬´–ü–ò–ö-–ö–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏¬ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏...</td>\n",
       "      <td>A</td>\n",
       "      <td>A+</td>\n",
       "      <td>–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –§–∞–∫—Ç–æ—Ä—ã, –æ–ø—Ä...</td>\n",
       "      <td>–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã–π –¥–µ–π—Å—Ç–≤–∏–µ —Ñ–∞–∫—Ç–æ—Ä –æ–ø—Ä–µ–¥–µ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>1017</td>\n",
       "      <td>–ù–ö–† –ø—Ä–∏—Å–≤–æ–∏–ª–æ –≥—Ä—É–ø–ø–µ –∫–æ–º–ø–∞–Ω–∏–π ¬´–ü–ò–ö¬ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π ...</td>\n",
       "      <td>A</td>\n",
       "      <td>A+</td>\n",
       "      <td>–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –§–∞–∫—Ç–æ—Ä—ã, –æ–ø—Ä...</td>\n",
       "      <td>–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã–π –¥–µ–π—Å—Ç–≤–∏–µ —Ñ–∞–∫—Ç–æ—Ä –æ–ø—Ä–µ–¥–µ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1160 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                             pr_txt –ö–∞—Ç–µ–≥–æ—Ä–∏—è  \\\n",
       "0        2  ¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–æ–º–ø...        BB   \n",
       "1        4  ¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø—Ä–∏—Å–≤–æ–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –ü–ê–û ¬´–§...       AAA   \n",
       "2        7  ¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–≤—ã—Å–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –û–ê–û ¬´–ú–†...        AA   \n",
       "3        8  ¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–Ω–∏–∑–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –ü–ê–û ¬´–ú....         A   \n",
       "4        9  ¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–≤—ã—Å–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–æ–º–ø–∞–Ω–∏...        BB   \n",
       "...    ...                                                ...       ...   \n",
       "1155   880  –ù–ö–† –ø—Ä–∏—Å–≤–æ–∏–ª–æ –≥—Ä—É–ø–ø–µ \"–†–ö–° –î–µ–≤–µ–ª–æ–ø–º–µ–Ω—Ç\" –∫—Ä–µ–¥–∏—Ç–Ω...       BBB   \n",
       "1156   908  –ù–ö–† –ø—Ä–∏—Å–≤–æ–∏–ª–æ –∑–∞–≤–æ–¥—É –ö—Ä–∏–∞–ª–≠–Ω–µ—Ä–≥–æ–°—Ç—Ä–æ–π –∫—Ä–µ–¥–∏—Ç–Ω—ã...       BBB   \n",
       "1157   978  –ù–ö–† –ø—Ä–∏—Å–≤–æ–∏–ª–æ –≥—Ä—É–ø–ø–µ ¬´–°–∞–º–æ–ª–µ—Ç¬ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏...         A   \n",
       "1158  1016  –ù–ö–† –ø—Ä–∏—Å–≤–æ–∏–ª–æ ¬´–ü–ò–ö-–ö–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏¬ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏...         A   \n",
       "1159  1017  –ù–ö–† –ø—Ä–∏—Å–≤–æ–∏–ª–æ –≥—Ä—É–ø–ø–µ –∫–æ–º–ø–∞–Ω–∏–π ¬´–ü–ò–ö¬ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π ...         A   \n",
       "\n",
       "     –£—Ä–æ–≤–µ–Ω—å —Ä–µ–π—Ç–∏–Ω–≥–∞                                     pr_txt_cleaned  \\\n",
       "0                  BB  ¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–æ–º–ø...   \n",
       "1                 AAA  ¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø—Ä–∏—Å–≤–æ–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –ü–ê–û ¬´–§...   \n",
       "2                 AA+  ¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–≤—ã—Å–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –û–ê–û ¬´–ú–†...   \n",
       "3                   A  ¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–Ω–∏–∑–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –ü–ê–û ¬´–ú....   \n",
       "4                 BB+  ¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–≤—ã—Å–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–æ–º–ø–∞–Ω–∏...   \n",
       "...               ...                                                ...   \n",
       "1155             BBB-  –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –§–∞–∫—Ç–æ—Ä—ã, –æ–ø—Ä...   \n",
       "1156             BBB-  –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –§–∞–∫—Ç–æ—Ä—ã, –æ–ø—Ä...   \n",
       "1157               A-  –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –§–∞–∫—Ç–æ—Ä—ã, –æ–ø—Ä...   \n",
       "1158               A+  –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –§–∞–∫—Ç–æ—Ä—ã, –æ–ø—Ä...   \n",
       "1159               A+  –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –§–∞–∫—Ç–æ—Ä—ã, –æ–ø—Ä...   \n",
       "\n",
       "                                      sentences_cleaned  \n",
       "0     –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–æ–º–ø–∞–Ω–∏—è —É—Ä–æ–≤–µ–Ω—å...  \n",
       "1     –ø—Ä–∏—Å–≤–æ–∏—Ç—å –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ —É—Ä–æ–≤–µ–Ω—å –º–∞—Ä—Ç —Ä–µ–π—Ç–∏...  \n",
       "2     –ø–æ–≤—ã—Å–∏—Ç—å –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ —É—Ä–æ–≤–µ–Ω—å –º–∞—Ä—Ç —Ä–µ–π—Ç–∏–Ω...  \n",
       "3     –ø–æ–Ω–∏–∑–∏—Ç—å –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ —É—Ä–æ–≤–µ–Ω—å –∏–∑–º–µ–Ω–∏—Ç—å –ø—Ä...  \n",
       "4     –ø–æ–≤—ã—Å–∏—Ç—å –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–æ–º–ø–∞–Ω–∏—è —É—Ä–æ–≤–µ–Ω—å –∏–∑...  \n",
       "...                                                 ...  \n",
       "1155  –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã–π –¥–µ–π—Å—Ç–≤–∏–µ —Ñ–∞–∫—Ç–æ—Ä –æ–ø—Ä–µ–¥–µ...  \n",
       "1156  –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã–π –¥–µ–π—Å—Ç–≤–∏–µ —Ñ–∞–∫—Ç–æ—Ä –æ–ø—Ä–µ–¥–µ...  \n",
       "1157  –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã–π –¥–µ–π—Å—Ç–≤–∏–µ —Ñ–∞–∫—Ç–æ—Ä –æ–ø—Ä–µ–¥–µ...  \n",
       "1158  –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã–π –¥–µ–π—Å—Ç–≤–∏–µ —Ñ–∞–∫—Ç–æ—Ä –æ–ø—Ä–µ–¥–µ...  \n",
       "1159  –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã–π –¥–µ–π—Å—Ç–≤–∏–µ —Ñ–∞–∫—Ç–æ—Ä –æ–ø—Ä–µ–¥–µ...  \n",
       "\n",
       "[1160 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9UEBw0VvNap",
    "outputId": "b7fee093-900b-4caa-b695-8829f472c017"
   },
   "outputs": [],
   "source": [
    "# –®–∞–≥ 2: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TChyOBZVwAqk",
    "outputId": "870c51ae-5681-4749-8395-8e7750bc3763",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# –®–∞–≥ 3: –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ (rubert-tiny2)\n",
    "from transformers import AutoModelForSequenceClassification#, BertConfig\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('cointegrated/rubert-tiny2', num_labels=7)  # 7 –∫–ª–∞—Å—Å–æ–≤ —É—Ä–æ–≤–Ω—è —Ä–µ–π—Ç–∏–Ω–≥–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bzj939K2yUtu",
    "outputId": "3ce91810-f999-4d23-f946-7f88fde997a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(83828, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(2048, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-2): 3 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=312, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –®–∞–≥ 4: Fine-tuning –º–æ–¥–µ–ª–∏\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vjwSyeUCTM2C",
    "outputId": "40b8f75d-0a44-4e87-a85f-a1b1bfe99c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.1.dev0+gba5374836.d20240125)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "b07850732b5e43b2971e8e22411ac3f1",
      "8be92b5a3a364009b6af55286c91560b",
      "80207e4e50e2459984cf2beab036f66b",
      "37e17a11a1b74296a3ac0af683721623",
      "892587c0220b4b739ac6ac653483138f",
      "ae212bf556394b92aefa01a0c5274bc7",
      "acb512b19fee440880d3ec6864be4669",
      "2dbe750eb36f44afb572760c6361217e",
      "3dcbfc7b131c494facccd1467d5fde0f",
      "4fa69ef96038425196e202e85f65c419",
      "2aaeda6ccd094cc3996cce35823f695e"
     ]
    },
    "id": "DaIxewkxS96v",
    "outputId": "bffdb17a-67c5-47d1-f74c-e02bd78e8e9e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd67817a1bb341aa9a23d6361a58a3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(df[\"–ö–∞—Ç–µ–≥–æ—Ä–∏—è\"])\n",
    "df['labels'] = labels\n",
    "\n",
    "df = df[['pr_txt_cleaned', 'labels']]\n",
    "df.to_csv('CRA_train_1200.csv', index=False)\n",
    "full_dataset = load_dataset('csv', data_files='CRA_train_1200.csv')\n",
    "\n",
    "df.labels.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ScuK3bII1TX",
    "outputId": "a39c1351-061b-417f-e598-4eed5a14f223"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pr_txt_cleaned', 'labels'],\n",
       "        num_rows: 928\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pr_txt_cleaned', 'labels'],\n",
       "        num_rows: 232\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset = full_dataset['train'].train_test_split(test_size=0.2, seed = 17)\n",
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2lAzRDhiHf2P"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset # (dataset format for transformers lib) split train/test; feed to learning algorithm; fast fucntion apply - .map()\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding #  to dynamically pad the sentences to the longest length in a batch during collation\n",
    "# import evaluate # loading metric F1\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer # TrainingArguments and Trainer are helpful instead of using verbose vanilla pytorh training workflow\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "b75811eaa63f40f0bb9a6e908db56001",
      "cf10dc85f4054f42afc998c4610cc88f",
      "0d3007d64cea4a50bcf32012fee8d17d",
      "afdfc15ad67b4088821d3b2d650b88ce",
      "8dcd632c9cab4753bdfc157e692e7fca",
      "75254c71403847e49dfed51646a30bdf",
      "a3ff77b4fb7c45e7af060e8871e9c1c5",
      "da4fac23ccbf4c289bc27ede999c2be9",
      "e2b1dd5ac52f4f1795f3e39bc66b998b",
      "a7d656e93e0c4fbda2ea8226996f7e22",
      "6605ce14652346a182a31dbc69d6db9e",
      "5bbe8d24867a4cef9c899bf9b4667665",
      "4bb90ce8f6794cdc80008b0b348332a3",
      "8189b7338ace4f379bdbe77dc48b8e7e",
      "28f57120d6d84cc2a421a3815fe3d6fe",
      "71952a9a8c944a1783d38195eae1550b",
      "a05b2fe8e9b54d92a2dcbde4d727e66a",
      "63a7e6d20dd04a21bdd73f0c5045bd82",
      "7b4abf3093c441b98e280cd14256fdd0",
      "4944065d9f1544a39718986e2e9fe33d",
      "abdebdbc5bad41088115f35fb2f1d5d6",
      "673cb2e6bd2c4676ba47d1c072f3675c"
     ]
    },
    "id": "DaZQz3TvGZeu",
    "outputId": "ca5ab0b4-04e2-4091-e8f6-c85213c9f30a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d954b3a6ffc64eba9c0b02b2e595f824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/928 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851fcba69f7b44498c9d07d4fece96b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_tokenized = full_dataset.map(lambda examples: tokenizer(examples[\"pr_txt_cleaned\"], truncation = True, max_length=2048, padding='max_length'), batched=True)\n",
    "\n",
    "dataset_tokenized.set_format(type='torch', device=device)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, max_length = 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PvV0dDEtIJOU"
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "COeWhUIdjbh_"
   },
   "outputs": [],
   "source": [
    "class MulticlassTrainer(Trainer):\n",
    "    # inheriting from Trainer class to override vanilla cross-entropy loss with weighted cross-enthropy loss\n",
    "    def __init__(self, weights, *args, **kwds):\n",
    "        super().__init__(*args, **kwds)\n",
    "        self.weights = weights\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        #loss = cross_entropy(logits.squeeze(), labels.squeeze(), torch.from_numpy(self.weights))\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=self.weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        #loss = log_loss(labels.squeeze(), logits.squeeze(), sample_weight = self.weights)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cXFinPXgIP5w",
    "outputId": "5ef05b95-599a-4f9e-f3db-2492c9e1164f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14351/2817711593.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  f1_metric = load_metric(\"f1\", trust_remote_code=True)\n"
     ]
    }
   ],
   "source": [
    "f1_metric = load_metric(\"f1\", trust_remote_code=True)\n",
    "def compute_metrics(eval_pred):\n",
    "    # using macro f1\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmT-5W2kIThm",
    "outputId": "1c5081be-3d25-4e09-e81c-cbf759f4820f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    #output_dir=\"./drive/MyDrive/ML/Models/female_clothing_class\", # checkpoints are saved here\n",
    "    output_dir = \"./out\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs = 60,\n",
    "    warmup_steps=500,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ —Ä–∞–∑–æ–≥—Ä–µ–≤–∞\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./runs\",  # –ö–∞—Ç–∞–ª–æ–≥ –¥–ª—è –ª–æ–≥–æ–≤ TensorBoard\n",
    "    logging_steps=50,  # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 400 —à–∞–≥–æ–≤\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps = 1500,\n",
    "    save_total_limit = 10, # max number of model checkpoints to safe\n",
    "    load_best_model_at_end=True, # automatically loads the best model (save_steps and eval_steps must be multiples oh each other)\n",
    "    metric_for_best_model=\"f1\",  # –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "    #report_to='tensorboard', # it's also possible to report to weights & biases or other\n",
    "    greater_is_better=True  # –£–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –±–æ–ª—å—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ F1 –ª—É—á—à–µ\n",
    ")\n",
    "\n",
    "# weights for unbalanced classes\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(dataset_tokenized[\"train\"][\"labels\"].cpu().numpy()), y=dataset_tokenized[\"train\"][\"labels\"].cpu().numpy())\n",
    "WEIGHTS = torch.from_numpy(weights).float().to(device)\n",
    "\n",
    "trainer = MulticlassTrainer(\n",
    "    WEIGHTS,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks = [tb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PlCCgyMKJGhc",
    "outputId": "d48a88a4-74ec-40f1-9599-f952c65823b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.device # check the current device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CUDA memory: 79.169677734375\n",
      "Allocated CUDA memory: 0.10903215408325195\n",
      "Cached CUDA memory: 0.126953125\n"
     ]
    }
   ],
   "source": [
    "def print_cuda_memory():\n",
    "    print(\"Total CUDA memory:\", torch.cuda.get_device_properties(0).total_memory/(2**30))\n",
    "    print(\"Allocated CUDA memory:\", torch.cuda.memory_allocated()/(2**30))\n",
    "    print(\"Cached CUDA memory:\", torch.cuda.memory_reserved()/(2**30))\n",
    "\n",
    "print_cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "ihUkKpajJJJ8",
    "outputId": "6ae5a17b-3db2-4c85-a68c-d6ab6d9f5f82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6960' max='6960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6960/6960 15:02, Epoch 60/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.948400</td>\n",
       "      <td>1.946969</td>\n",
       "      <td>0.051673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.941200</td>\n",
       "      <td>1.935951</td>\n",
       "      <td>0.074785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.924800</td>\n",
       "      <td>1.915184</td>\n",
       "      <td>0.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.910500</td>\n",
       "      <td>1.892432</td>\n",
       "      <td>0.096755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.882400</td>\n",
       "      <td>1.866293</td>\n",
       "      <td>0.093800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.859500</td>\n",
       "      <td>1.849899</td>\n",
       "      <td>0.079631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.873400</td>\n",
       "      <td>1.832311</td>\n",
       "      <td>0.157778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.870300</td>\n",
       "      <td>1.810745</td>\n",
       "      <td>0.142780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.806600</td>\n",
       "      <td>1.793448</td>\n",
       "      <td>0.175713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.786400</td>\n",
       "      <td>1.751880</td>\n",
       "      <td>0.213680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.724100</td>\n",
       "      <td>1.686718</td>\n",
       "      <td>0.322488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.661900</td>\n",
       "      <td>1.616625</td>\n",
       "      <td>0.279644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.571700</td>\n",
       "      <td>1.553621</td>\n",
       "      <td>0.385868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.480500</td>\n",
       "      <td>1.498772</td>\n",
       "      <td>0.281670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.395900</td>\n",
       "      <td>1.404055</td>\n",
       "      <td>0.349685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.355900</td>\n",
       "      <td>1.423914</td>\n",
       "      <td>0.318739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.328800</td>\n",
       "      <td>1.326079</td>\n",
       "      <td>0.358516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.233800</td>\n",
       "      <td>1.291067</td>\n",
       "      <td>0.444199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.170100</td>\n",
       "      <td>1.264938</td>\n",
       "      <td>0.368059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.136300</td>\n",
       "      <td>1.231627</td>\n",
       "      <td>0.420424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.099700</td>\n",
       "      <td>1.182675</td>\n",
       "      <td>0.425793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.054200</td>\n",
       "      <td>1.207674</td>\n",
       "      <td>0.427276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.950700</td>\n",
       "      <td>1.123846</td>\n",
       "      <td>0.437221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.924600</td>\n",
       "      <td>1.135723</td>\n",
       "      <td>0.464675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.906600</td>\n",
       "      <td>1.068061</td>\n",
       "      <td>0.504998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.859100</td>\n",
       "      <td>1.046004</td>\n",
       "      <td>0.539024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.784400</td>\n",
       "      <td>1.015114</td>\n",
       "      <td>0.586193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.769100</td>\n",
       "      <td>1.011222</td>\n",
       "      <td>0.551180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.706700</td>\n",
       "      <td>0.976996</td>\n",
       "      <td>0.659255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.673500</td>\n",
       "      <td>0.953387</td>\n",
       "      <td>0.603239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.614000</td>\n",
       "      <td>0.931230</td>\n",
       "      <td>0.562281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.611500</td>\n",
       "      <td>1.017564</td>\n",
       "      <td>0.607628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.559200</td>\n",
       "      <td>0.928956</td>\n",
       "      <td>0.629998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>0.904721</td>\n",
       "      <td>0.657823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>0.885296</td>\n",
       "      <td>0.644187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.460200</td>\n",
       "      <td>0.880254</td>\n",
       "      <td>0.661713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.425600</td>\n",
       "      <td>0.868017</td>\n",
       "      <td>0.665853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.866018</td>\n",
       "      <td>0.657533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>0.882071</td>\n",
       "      <td>0.653802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>0.877198</td>\n",
       "      <td>0.660070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.316300</td>\n",
       "      <td>0.852994</td>\n",
       "      <td>0.653613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.309600</td>\n",
       "      <td>0.935668</td>\n",
       "      <td>0.657766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.858940</td>\n",
       "      <td>0.653526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.259000</td>\n",
       "      <td>0.942772</td>\n",
       "      <td>0.673681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.876243</td>\n",
       "      <td>0.665101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.196800</td>\n",
       "      <td>0.895084</td>\n",
       "      <td>0.679407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.203900</td>\n",
       "      <td>0.861790</td>\n",
       "      <td>0.678582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.168400</td>\n",
       "      <td>0.915333</td>\n",
       "      <td>0.668685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.171800</td>\n",
       "      <td>0.911529</td>\n",
       "      <td>0.685588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.153900</td>\n",
       "      <td>0.872140</td>\n",
       "      <td>0.694350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.134800</td>\n",
       "      <td>0.918291</td>\n",
       "      <td>0.635329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.133100</td>\n",
       "      <td>0.941641</td>\n",
       "      <td>0.682123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.896145</td>\n",
       "      <td>0.679693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.945246</td>\n",
       "      <td>0.687492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.091300</td>\n",
       "      <td>0.956682</td>\n",
       "      <td>0.674196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.995748</td>\n",
       "      <td>0.656739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.074200</td>\n",
       "      <td>0.964499</td>\n",
       "      <td>0.669742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.944394</td>\n",
       "      <td>0.680644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.999110</td>\n",
       "      <td>0.681862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>1.011241</td>\n",
       "      <td>0.681212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>0.992745</td>\n",
       "      <td>0.725091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.042600</td>\n",
       "      <td>0.996936</td>\n",
       "      <td>0.678460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>1.069615</td>\n",
       "      <td>0.669747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>1.036344</td>\n",
       "      <td>0.694423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>1.052615</td>\n",
       "      <td>0.706329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>1.094526</td>\n",
       "      <td>0.700891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>1.086242</td>\n",
       "      <td>0.700351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>1.062393</td>\n",
       "      <td>0.696970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>1.072438</td>\n",
       "      <td>0.724895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>1.085453</td>\n",
       "      <td>0.697694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>1.215194</td>\n",
       "      <td>0.677555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>1.120842</td>\n",
       "      <td>0.708158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>1.132747</td>\n",
       "      <td>0.713051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>1.103480</td>\n",
       "      <td>0.711894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>1.174629</td>\n",
       "      <td>0.710511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>1.117719</td>\n",
       "      <td>0.714692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>1.141623</td>\n",
       "      <td>0.698822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>1.186997</td>\n",
       "      <td>0.718414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>1.253987</td>\n",
       "      <td>0.706258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>1.223882</td>\n",
       "      <td>0.706458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>1.186761</td>\n",
       "      <td>0.711701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>1.160434</td>\n",
       "      <td>0.716710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>1.191209</td>\n",
       "      <td>0.720243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>1.281317</td>\n",
       "      <td>0.710868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>1.229551</td>\n",
       "      <td>0.706835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>1.302972</td>\n",
       "      <td>0.709313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.232821</td>\n",
       "      <td>0.711216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>1.279267</td>\n",
       "      <td>0.708927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>1.280155</td>\n",
       "      <td>0.723229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>1.232321</td>\n",
       "      <td>0.722507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>1.268161</td>\n",
       "      <td>0.717509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>1.299464</td>\n",
       "      <td>0.730404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>1.275996</td>\n",
       "      <td>0.716426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>1.268164</td>\n",
       "      <td>0.715390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.293277</td>\n",
       "      <td>0.703429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>1.232712</td>\n",
       "      <td>0.727032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>1.263775</td>\n",
       "      <td>0.724201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.271680</td>\n",
       "      <td>0.722038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.256532</td>\n",
       "      <td>0.731660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.304229</td>\n",
       "      <td>0.725764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.319675</td>\n",
       "      <td>0.721165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.302408</td>\n",
       "      <td>0.729957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.344887</td>\n",
       "      <td>0.715207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.321840</td>\n",
       "      <td>0.728231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.284096</td>\n",
       "      <td>0.740415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>1.298148</td>\n",
       "      <td>0.734966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.343130</td>\n",
       "      <td>0.709201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.288743</td>\n",
       "      <td>0.727421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>1.336564</td>\n",
       "      <td>0.733095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.277411</td>\n",
       "      <td>0.730837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.321827</td>\n",
       "      <td>0.727460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.392890</td>\n",
       "      <td>0.724998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>1.318539</td>\n",
       "      <td>0.728589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>1.305366</td>\n",
       "      <td>0.724192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.340555</td>\n",
       "      <td>0.730477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>1.331718</td>\n",
       "      <td>0.704983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.356759</td>\n",
       "      <td>0.728589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.383880</td>\n",
       "      <td>0.716287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.323204</td>\n",
       "      <td>0.738164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.315637</td>\n",
       "      <td>0.727577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.332646</td>\n",
       "      <td>0.724192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.393921</td>\n",
       "      <td>0.713090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.386978</td>\n",
       "      <td>0.715002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.361602</td>\n",
       "      <td>0.726592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.393605</td>\n",
       "      <td>0.722237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.364102</td>\n",
       "      <td>0.733899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>1.340494</td>\n",
       "      <td>0.729712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.344029</td>\n",
       "      <td>0.732807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.348989</td>\n",
       "      <td>0.732850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.370306</td>\n",
       "      <td>0.732165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.363428</td>\n",
       "      <td>0.731880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.357179</td>\n",
       "      <td>0.728850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>1.353553</td>\n",
       "      <td>0.731880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.348978</td>\n",
       "      <td>0.726975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.338210</td>\n",
       "      <td>0.736502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>1.369112</td>\n",
       "      <td>0.728850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.375171</td>\n",
       "      <td>0.728850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.374041</td>\n",
       "      <td>0.728850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.375900</td>\n",
       "      <td>0.728850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6960, training_loss=0.36187212182390877, metrics={'train_runtime': 902.7102, 'train_samples_per_second': 61.681, 'train_steps_per_second': 7.71, 'total_flos': 1643454144184320.0, 'train_loss': 0.36187212182390877, 'epoch': 60.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test = pd.read_excel('clean_test.xlsx')\n",
    "df_test = pd.read_excel('CRA_validation_110_answers.xlsx')\n",
    "#X_test = df_test['pr_txt_cleaned']  # –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏\n",
    "#y_test = df_test['–ö–∞—Ç–µ–≥–æ—Ä–∏—è']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "byxbxgPLMw34"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d4cc9369144b9fa7a65e4bcdaa2075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(df_test[\"–ö–∞—Ç–µ–≥–æ—Ä–∏—è\"])\n",
    "df_test['labels'] = labels\n",
    "\n",
    "df_test = df_test[['pr_txt', 'labels']]\n",
    "df_test.to_csv('CRA_test_110_answers.csv', index=False)\n",
    "test_dataset = load_dataset('csv', data_files='CRA_test_110_answers.csv')\n",
    "# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–ø–∏—Å–µ–π, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è –∏–ª–∏ –º–µ—Ç–∫–∏ –Ω–µ —è–≤–ª—è—é—Ç—Å—è None\n",
    "#full_dataset = full_dataset.filter(lambda example: example['Review Text'] is not None and example['Recommended IND'] is not None)\n",
    "\n",
    "df_test.labels.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16624ea739547d1b215986d267d9031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset_tokenized = test_dataset.map(lambda examples: tokenizer(examples[\"pr_txt\"], truncation = True, max_length=2048, padding='max_length'), batched=True)\n",
    "# automatically truncate long sentences to be no longer than DistilBERT‚Äôs maximum input length\n",
    "\n",
    "test_dataset_tokenized.set_format(type='torch', device=device)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, max_length = 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(test_dataset_tokenized['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-2.12807202e+00, -3.90787911e+00, -5.44260144e-01,\n",
       "        -4.85247932e-02,  6.71468115e+00, -9.43976760e-01,\n",
       "        -9.92004871e-01],\n",
       "       [ 5.63018918e-01,  6.64142084e+00, -1.81935179e+00,\n",
       "        -1.91029751e+00, -2.99364018e+00,  1.15416765e+00,\n",
       "        -7.17695951e-01],\n",
       "       [-1.87608635e+00, -4.21275616e+00, -1.17727768e+00,\n",
       "        -9.75859523e-01,  6.92421532e+00,  5.39640546e-01,\n",
       "        -1.30190873e+00],\n",
       "       [ 6.38750553e+00, -1.65200698e+00, -3.09478998e+00,\n",
       "        -9.89487886e-01,  1.52845159e-01, -5.23266017e-01,\n",
       "        -1.39624262e+00],\n",
       "       [ 4.16835690e+00, -3.29812169e+00, -2.53669095e+00,\n",
       "        -3.67311716e-01,  2.70940900e+00, -6.74333096e-01,\n",
       "        -1.74616790e+00],\n",
       "       [ 1.49865985e+00, -4.06721687e+00, -3.61190009e+00,\n",
       "        -1.37240851e+00,  3.90869379e+00,  2.75568247e+00,\n",
       "        -1.22052324e+00],\n",
       "       [ 8.65659595e-01,  7.44091320e+00, -7.00417042e-01,\n",
       "        -1.88672578e+00, -3.80693579e+00,  3.35606724e-01,\n",
       "        -8.59155357e-01],\n",
       "       [ 7.16958761e+00, -1.35680532e+00, -2.02948260e+00,\n",
       "        -1.24253476e+00, -1.04573679e+00, -9.11866486e-01,\n",
       "        -1.19547701e+00],\n",
       "       [-9.81354177e-01,  5.40336311e-01,  7.49807596e+00,\n",
       "        -5.76259375e-01, -9.88641322e-01, -3.48415589e+00,\n",
       "        -3.77265424e-01],\n",
       "       [-1.22172153e+00, -5.52443862e-01, -3.42660570e+00,\n",
       "        -2.30767393e+00, -9.88342315e-02,  7.15525055e+00,\n",
       "        -4.88492250e-01],\n",
       "       [ 2.73463935e-01, -2.48111629e+00, -4.02096844e+00,\n",
       "        -2.21592712e+00,  1.73980653e+00,  6.34609175e+00,\n",
       "        -1.23271644e+00],\n",
       "       [ 7.14078331e+00, -1.16846859e+00, -1.70882452e+00,\n",
       "        -1.19605398e+00, -1.18919265e+00, -1.21325898e+00,\n",
       "        -1.18086898e+00],\n",
       "       [ 5.66573095e+00, -2.76385093e+00, -3.34114933e+00,\n",
       "        -1.50389910e-01,  9.75966871e-01, -7.04308689e-01,\n",
       "        -1.09230840e+00],\n",
       "       [ 6.95774841e+00, -8.16547871e-01, -2.88564968e+00,\n",
       "        -1.06853116e+00, -7.04651415e-01, -8.70616198e-01,\n",
       "        -1.47255790e+00],\n",
       "       [-1.60479414e+00, -2.36186910e+00, -3.26259327e+00,\n",
       "        -1.99760377e+00,  2.79284120e+00,  5.59792233e+00,\n",
       "        -6.24692142e-01],\n",
       "       [ 6.22098970e+00, -1.75817943e+00, -3.06721663e+00,\n",
       "        -2.10079622e+00, -5.12956738e-01,  1.57989931e+00,\n",
       "        -1.50564969e+00],\n",
       "       [ 7.00024891e+00, -1.07808936e+00, -1.38325822e+00,\n",
       "        -1.03904366e+00, -1.11033118e+00, -1.31733072e+00,\n",
       "        -1.63549089e+00],\n",
       "       [ 6.94852495e+00, -1.11223304e+00, -2.72025728e+00,\n",
       "        -1.16867948e+00, -3.36937845e-01, -8.96691799e-01,\n",
       "        -1.63223803e+00],\n",
       "       [ 6.26181269e+00, -1.78468502e+00, -2.10836411e+00,\n",
       "         2.64706016e-02, -3.89034957e-01, -1.89616489e+00,\n",
       "        -8.80768716e-01],\n",
       "       [ 6.29797029e+00, -2.37945127e+00, -2.80134511e+00,\n",
       "        -9.82203066e-01,  3.39138091e-01, -3.56409729e-01,\n",
       "        -1.28624892e+00],\n",
       "       [-2.14937901e+00, -4.29946280e+00, -1.37150705e+00,\n",
       "        -7.07908154e-01,  6.95976734e+00,  6.60247087e-01,\n",
       "        -1.17283404e+00],\n",
       "       [-2.84130597e+00,  1.10950387e+00,  7.46742964e+00,\n",
       "        -6.55649126e-01, -4.55709904e-01, -2.84410024e+00,\n",
       "        -1.05770910e-03],\n",
       "       [-1.14270067e+00, -1.25176072e+00, -1.18582845e+00,\n",
       "         6.41860485e+00, -6.88414097e-01, -2.06208110e+00,\n",
       "        -5.98805487e-01],\n",
       "       [ 5.37207985e+00, -2.42040038e+00, -3.27683401e+00,\n",
       "        -7.25884557e-01,  9.22991395e-01, -5.04850924e-01,\n",
       "        -7.77678967e-01],\n",
       "       [ 4.94868231e+00, -2.14868808e+00, -2.63373899e+00,\n",
       "         7.94578418e-02,  1.59773791e+00, -1.71053672e+00,\n",
       "        -1.46579266e+00],\n",
       "       [-1.19388628e+00, -1.72720861e+00, -1.09391201e+00,\n",
       "         6.37019920e+00,  2.02961087e-01, -2.56066298e+00,\n",
       "        -7.73757994e-01],\n",
       "       [-1.96593070e+00,  3.18165980e-02,  7.63296986e+00,\n",
       "        -5.08124113e-01, -3.83746058e-01, -2.84755254e+00,\n",
       "        -3.59171808e-01],\n",
       "       [-8.19905639e-01, -1.67267954e+00, -1.14587986e+00,\n",
       "         6.41256809e+00, -6.77521825e-01, -2.08544326e+00,\n",
       "        -6.42529488e-01],\n",
       "       [-5.56521475e-01, -4.39048672e+00, -1.85336959e+00,\n",
       "         5.30802786e-01,  6.19163227e+00, -9.44230497e-01,\n",
       "        -1.16052234e+00],\n",
       "       [ 7.02727222e+00, -7.90164709e-01, -3.33185673e+00,\n",
       "        -2.01481080e+00, -1.16580379e+00,  6.73853040e-01,\n",
       "        -1.23920465e+00],\n",
       "       [-2.15095803e-01,  7.54979229e+00,  9.02211443e-02,\n",
       "        -1.60077620e+00, -3.37425065e+00, -3.95626158e-01,\n",
       "        -7.41927087e-01],\n",
       "       [ 4.81817865e+00, -1.99570549e+00, -1.74499035e+00,\n",
       "        -6.83623731e-01,  7.11916685e-01, -5.64722300e-01,\n",
       "        -1.66968942e+00],\n",
       "       [-1.66626906e+00, -3.94046044e+00, -1.44323146e+00,\n",
       "        -1.30615234e+00,  6.72211981e+00,  9.94641960e-01,\n",
       "        -1.45607340e+00],\n",
       "       [ 7.65131354e-01,  6.41443014e+00, -1.60378277e+00,\n",
       "        -1.90776861e+00, -2.59958434e+00,  3.54632080e-01,\n",
       "        -5.12795508e-01],\n",
       "       [ 6.22403288e+00, -1.25897563e+00, -2.04163432e+00,\n",
       "         4.38198149e-01, -8.56997669e-01, -1.76195192e+00,\n",
       "        -1.55752838e+00],\n",
       "       [-1.44727826e+00, -1.71116221e+00, -3.30923033e+00,\n",
       "        -2.45555305e+00,  2.25614858e+00,  6.07615137e+00,\n",
       "        -8.27986062e-01],\n",
       "       [ 6.98366404e+00, -1.37862360e+00, -2.44840932e+00,\n",
       "        -1.09999216e+00, -7.21290529e-01, -8.86772811e-01,\n",
       "        -1.18195200e+00],\n",
       "       [-2.57671547e+00,  1.09377873e+00,  7.56552982e+00,\n",
       "        -6.80364847e-01, -4.98454332e-01, -2.91020060e+00,\n",
       "        -2.35751525e-01],\n",
       "       [-1.46005857e+00, -4.12900686e-01, -3.66461873e+00,\n",
       "        -2.46661782e+00,  8.53673995e-01,  6.80976629e+00,\n",
       "        -7.66221464e-01],\n",
       "       [-1.41423762e+00,  1.96867800e+00,  7.25017595e+00,\n",
       "        -7.86407709e-01, -1.38650560e+00, -3.73715878e+00,\n",
       "        -3.40726227e-02],\n",
       "       [-3.93704891e-01,  7.52833748e+00, -5.23872674e-01,\n",
       "        -1.38977730e+00, -3.59356070e+00,  2.67548293e-01,\n",
       "        -5.99218071e-01],\n",
       "       [ 7.05666065e+00, -1.65968668e+00, -2.54627013e+00,\n",
       "        -1.46362162e+00, -8.66319537e-01, -1.67508319e-01,\n",
       "        -1.14939702e+00],\n",
       "       [ 6.91368437e+00, -6.01850271e-01, -2.91564441e+00,\n",
       "        -2.52571797e+00, -9.56065595e-01,  8.46685290e-01,\n",
       "        -1.63313937e+00],\n",
       "       [-2.25618690e-01, -5.13249826e+00, -2.49158430e+00,\n",
       "        -7.47764587e-01,  6.04859257e+00,  1.36550653e+00,\n",
       "        -1.22021890e+00],\n",
       "       [ 6.45446491e+00, -1.44909191e+00, -3.42585492e+00,\n",
       "        -1.82337058e+00, -6.63235486e-01,  1.30998814e+00,\n",
       "        -1.49700201e+00],\n",
       "       [-1.49851942e+00, -2.74574065e+00, -3.11715198e+00,\n",
       "        -2.19690180e+00,  3.76877880e+00,  4.93213129e+00,\n",
       "        -7.71680415e-01],\n",
       "       [ 5.88950348e+00,  9.64062154e-01, -1.96719632e-01,\n",
       "        -9.07418728e-01, -1.90220761e+00, -2.17439461e+00,\n",
       "        -1.61129463e+00],\n",
       "       [ 4.97582483e+00, -3.21123457e+00, -2.08603382e+00,\n",
       "        -9.12101805e-01,  2.38393044e+00, -1.35851550e+00,\n",
       "        -1.27468646e+00],\n",
       "       [ 6.70378590e+00, -1.67301559e+00, -3.08142877e+00,\n",
       "        -1.27580249e+00, -4.61596549e-01, -1.02581652e-02,\n",
       "        -1.16790700e+00],\n",
       "       [-9.14610803e-01, -1.27610695e+00, -3.42332816e+00,\n",
       "        -2.53327847e+00,  6.13179445e-01,  7.03981876e+00,\n",
       "        -6.45986021e-01],\n",
       "       [-1.30907357e+00, -1.68231809e+00, -3.23403192e+00,\n",
       "        -2.32022643e+00,  1.62277448e+00,  6.47484970e+00,\n",
       "        -8.21612120e-01],\n",
       "       [-2.31742716e+00,  1.94425896e-01,  7.66714287e+00,\n",
       "        -5.95614672e-01, -2.55897611e-01, -2.87714243e+00,\n",
       "        -1.72611043e-01],\n",
       "       [-1.01486897e+00, -2.42962450e-01, -3.59954834e+00,\n",
       "        -2.18069410e+00, -2.43161306e-01,  7.11633539e+00,\n",
       "        -7.99926817e-01],\n",
       "       [-1.72508657e+00, -3.19531536e+00, -2.15141249e+00,\n",
       "        -2.15489244e+00,  4.82855844e+00,  3.86888433e+00,\n",
       "        -1.28154123e+00],\n",
       "       [ 6.58517075e+00, -1.41862643e+00, -2.04963326e+00,\n",
       "        -3.87950629e-01, -6.52132750e-01, -1.49356198e+00,\n",
       "        -1.33224475e+00],\n",
       "       [-1.15013015e+00, -2.15301943e+00, -3.34093404e+00,\n",
       "        -2.34247041e+00,  1.91166711e+00,  6.44536638e+00,\n",
       "        -7.23972559e-01],\n",
       "       [-1.84614134e+00, -3.30077380e-01,  7.54410934e+00,\n",
       "        -3.40332508e-01, -3.17623526e-01, -2.87351203e+00,\n",
       "        -2.87292838e-01],\n",
       "       [-1.73967195e+00, -5.30141532e-01,  7.53812504e+00,\n",
       "        -5.31853974e-01, -1.14312947e-01, -2.77804351e+00,\n",
       "        -3.55148494e-01],\n",
       "       [ 4.60237217e+00,  1.19737411e+00, -2.97969913e+00,\n",
       "        -2.54261208e+00, -3.12078333e+00,  3.26383495e+00,\n",
       "        -1.89571112e-01],\n",
       "       [ 1.15529567e-01,  6.61807251e+00, -1.61317909e+00,\n",
       "        -2.03951645e+00, -3.26873112e+00,  1.93000758e+00,\n",
       "        -6.09198689e-01],\n",
       "       [ 6.72417307e+00, -8.00749958e-01, -3.21016121e+00,\n",
       "        -1.01723182e+00, -7.57502079e-01, -5.79867661e-01,\n",
       "        -1.28027296e+00],\n",
       "       [-1.30636573e-01,  2.65651560e+00,  6.32047701e+00,\n",
       "        -1.42880678e+00, -1.62444794e+00, -3.16552806e+00,\n",
       "        -1.23817825e+00],\n",
       "       [-1.70202684e+00, -1.07393503e+00, -3.40774083e+00,\n",
       "        -2.28599238e+00,  1.44150293e+00,  6.55807924e+00,\n",
       "        -7.02437699e-01],\n",
       "       [-2.04566383e+00,  2.60635046e-04,  7.65035295e+00,\n",
       "        -5.51732838e-01, -2.07970724e-01, -2.81909108e+00,\n",
       "        -4.54324335e-01],\n",
       "       [-1.00377774e+00, -1.52518797e+00, -3.52030396e+00,\n",
       "        -2.50640273e+00,  1.09582436e+00,  6.96024609e+00,\n",
       "        -7.58572817e-01],\n",
       "       [ 7.04843426e+00, -2.54932761e-01, -2.19123435e+00,\n",
       "        -2.11356831e+00, -1.38537753e+00, -6.07026398e-01,\n",
       "        -1.06793833e+00],\n",
       "       [ 6.43947792e+00, -1.53206074e+00, -5.16414940e-01,\n",
       "        -4.67913002e-01, -1.42907417e+00, -1.75541222e+00,\n",
       "        -1.11747766e+00],\n",
       "       [-9.25186276e-01, -9.75740552e-01, -3.44000554e+00,\n",
       "        -2.70575476e+00,  6.59598112e-01,  7.04805946e+00,\n",
       "        -8.15768898e-01],\n",
       "       [-1.02486610e+00, -1.69458401e+00, -3.24550366e+00,\n",
       "        -2.85524774e+00,  3.43311429e+00,  5.19332886e+00,\n",
       "        -1.48425770e+00],\n",
       "       [ 9.11011267e-03, -5.27090216e+00, -2.31444860e+00,\n",
       "        -2.00100794e-01,  6.17564869e+00,  2.72533804e-01,\n",
       "        -1.10176170e+00],\n",
       "       [ 1.76609099e-01, -1.32127774e+00, -4.29169750e+00,\n",
       "        -2.76094508e+00,  1.58171260e+00,  6.61744499e+00,\n",
       "        -1.49203587e+00],\n",
       "       [ 6.32691956e+00, -1.97037876e+00, -3.16816306e+00,\n",
       "        -1.43732011e+00,  4.40473389e-03,  2.45555654e-01,\n",
       "        -1.23531592e+00],\n",
       "       [-2.04631066e+00,  1.55933738e-01,  7.57619762e+00,\n",
       "        -4.96775806e-01, -3.91981214e-01, -2.85805130e+00,\n",
       "        -3.46906841e-01],\n",
       "       [ 9.26861644e-01,  7.35389423e+00, -1.05723572e+00,\n",
       "        -1.63543141e+00, -3.86903477e+00,  4.11858708e-01,\n",
       "        -8.35846126e-01],\n",
       "       [ 6.09924221e+00, -2.54157996e+00, -2.72425056e+00,\n",
       "        -2.08215654e-01,  4.88355011e-01, -1.04030442e+00,\n",
       "        -1.19865084e+00],\n",
       "       [ 1.65568972e+00, -1.82464921e+00, -3.85337496e+00,\n",
       "        -2.25797629e+00,  2.98843908e+00,  2.52351189e+00,\n",
       "        -9.96216953e-01],\n",
       "       [ 4.89084148e+00, -3.11438799e+00, -2.87154984e+00,\n",
       "        -1.35382450e+00,  2.08798313e+00,  1.25342309e-01,\n",
       "        -1.47748101e+00],\n",
       "       [ 4.71953106e+00, -6.60791457e-01, -3.28589249e+00,\n",
       "        -2.25957584e+00, -2.07395840e+00,  3.41746044e+00,\n",
       "        -2.28069291e-01],\n",
       "       [-1.99356771e+00,  1.36074051e-01,  7.68288136e+00,\n",
       "        -5.25250912e-01, -4.26660657e-01, -2.94225788e+00,\n",
       "        -2.84395128e-01],\n",
       "       [-1.05341995e+00,  2.55038023e-01,  7.46556091e+00,\n",
       "        -8.70446444e-01, -7.04846680e-01, -3.17778563e+00,\n",
       "        -3.74881297e-01],\n",
       "       [-1.71222878e+00, -2.90360856e+00, -2.59370542e+00,\n",
       "        -2.27638054e+00,  4.39673233e+00,  4.24559879e+00,\n",
       "        -8.95357192e-01],\n",
       "       [-1.73649359e+00, -4.23841524e+00, -9.68548119e-01,\n",
       "        -4.29490924e-01,  6.93512011e+00, -3.26256990e-01,\n",
       "        -1.27331054e+00],\n",
       "       [ 3.93990421e+00,  5.03654003e+00, -1.21414877e-01,\n",
       "        -1.53010726e+00, -4.50633287e+00, -9.62800324e-01,\n",
       "        -7.53539622e-01],\n",
       "       [-2.04805732e+00,  3.56015205e-01, -2.96208644e+00,\n",
       "        -2.04747868e+00, -4.17899311e-01,  6.78583670e+00,\n",
       "        -3.89171541e-01],\n",
       "       [-2.39506796e-01,  7.01117945e+00, -1.06711411e+00,\n",
       "        -2.00616741e+00, -3.15429688e+00,  1.33513713e+00,\n",
       "        -6.97035372e-01],\n",
       "       [-9.55076396e-01, -1.27992356e+00, -3.45875645e+00,\n",
       "        -2.65795088e+00,  7.64159322e-01,  7.03485537e+00,\n",
       "        -6.35373116e-01],\n",
       "       [-2.20999455e+00, -4.28858089e+00, -9.76773441e-01,\n",
       "        -8.87153625e-01,  6.86924124e+00,  6.24533117e-01,\n",
       "        -1.16583788e+00],\n",
       "       [-1.40196669e+00, -1.04497038e-01,  7.56081581e+00,\n",
       "        -4.91715938e-01, -4.11375076e-01, -3.13598728e+00,\n",
       "        -4.76854056e-01],\n",
       "       [-1.83980691e+00, -4.07343268e-01,  7.58130598e+00,\n",
       "        -4.47424918e-01, -1.20813482e-01, -2.77674866e+00,\n",
       "        -4.92290705e-01],\n",
       "       [-1.53964245e+00,  4.92792964e-01,  7.50153971e+00,\n",
       "        -7.12549746e-01, -6.74775064e-01, -3.19256091e+00,\n",
       "        -2.02060223e-01],\n",
       "       [ 7.24623156e+00, -1.08322144e+00, -2.32278490e+00,\n",
       "        -1.57726717e+00, -1.08099365e+00, -6.20255053e-01,\n",
       "        -1.26803315e+00],\n",
       "       [-1.47430182e+00,  2.98822880e-01,  7.63832808e+00,\n",
       "        -4.82333779e-01, -6.04330957e-01, -3.19593740e+00,\n",
       "        -5.22556424e-01],\n",
       "       [-1.27007222e+00, -1.09987557e+00, -3.53838134e+00,\n",
       "        -2.41178894e+00,  9.56689060e-01,  6.88485098e+00,\n",
       "        -6.89152062e-01],\n",
       "       [-3.42185557e-01,  7.07800245e+00, -9.99287963e-01,\n",
       "        -1.91554415e+00, -2.91435432e+00,  8.86103153e-01,\n",
       "        -6.89902306e-01],\n",
       "       [-8.81396294e-01, -1.14384902e+00, -3.37512255e+00,\n",
       "        -2.61782384e+00,  4.04044807e-01,  7.13160324e+00,\n",
       "        -6.59787595e-01],\n",
       "       [-2.30719829e+00, -4.02494478e+00, -6.78658724e-01,\n",
       "        -4.63667274e-01,  6.87229252e+00, -2.50634432e-01,\n",
       "        -1.08252347e+00],\n",
       "       [-4.71482664e-01,  7.57815075e+00, -1.05732314e-01,\n",
       "        -1.51583707e+00, -3.48993802e+00, -1.38435364e-01,\n",
       "        -4.91179466e-01],\n",
       "       [-1.42400503e+00,  7.27549505e+00,  1.67153656e-01,\n",
       "        -1.27566683e+00, -3.18992424e+00,  1.17161423e-01,\n",
       "        -3.83214653e-01],\n",
       "       [ 6.49531555e+00,  1.56567669e+00, -3.82993937e+00,\n",
       "        -1.75868917e+00, -2.06939697e+00,  4.54695106e-01,\n",
       "        -1.34676492e+00],\n",
       "       [-2.08997512e+00, -4.01082945e+00, -9.93234992e-01,\n",
       "        -6.27858222e-01,  6.95621157e+00, -8.49850774e-02,\n",
       "        -1.14693415e+00],\n",
       "       [ 3.12191606e+00,  1.22202635e+00, -2.69812894e+00,\n",
       "        -1.53338373e+00, -2.49879026e+00, -1.77274048e-01,\n",
       "         3.59782481e+00],\n",
       "       [-5.65587521e-01,  7.32044697e+00, -2.96435863e-01,\n",
       "        -1.88012934e+00, -3.72104979e+00,  8.10071588e-01,\n",
       "        -1.38291836e-01],\n",
       "       [-8.20125282e-01, -1.44397950e+00, -3.46678066e+00,\n",
       "        -2.28553534e+00, -5.45428172e-02,  7.03112364e+00,\n",
       "         7.54073188e-02],\n",
       "       [ 6.12578440e+00, -5.61163545e-01, -3.49747682e+00,\n",
       "        -1.63248110e+00, -1.90204918e+00,  4.70527798e-01,\n",
       "         7.11287022e-01],\n",
       "       [ 1.47653484e+00,  4.39254045e+00,  9.67787266e-01,\n",
       "        -2.18681288e+00, -3.56643629e+00, -1.92096984e+00,\n",
       "         2.79223776e+00],\n",
       "       [ 2.19361234e+00,  4.66375208e+00, -1.41777802e+00,\n",
       "        -2.46881270e+00, -3.74004078e+00, -1.83884934e-01,\n",
       "         2.62311721e+00],\n",
       "       [ 1.86506915e+00, -2.00422382e+00, -2.26996112e+00,\n",
       "        -2.34011841e+00,  3.88833910e-01,  5.84587991e-01,\n",
       "         4.04796028e+00],\n",
       "       [-3.97682875e-01,  3.98637891e-01, -3.87924910e+00,\n",
       "        -2.67433691e+00, -8.57946277e-01,  6.55510283e+00,\n",
       "         3.99885118e-01],\n",
       "       [ 4.87686682e+00,  6.82045221e-01, -4.75735378e+00,\n",
       "        -2.17581391e+00, -2.22482061e+00,  3.04844093e+00,\n",
       "         3.01325232e-01],\n",
       "       [ 1.21174252e+00,  4.51511502e-01, -4.55268860e+00,\n",
       "        -2.58878112e+00, -1.13532531e+00,  5.46092033e+00,\n",
       "         7.63074875e-01]], dtype=float32), label_ids=array([4, 1, 4, 0, 0, 3, 1, 0, 2, 5, 5, 0, 0, 0, 5, 0, 0, 1, 0, 0, 4, 2,\n",
       "       3, 0, 0, 3, 2, 3, 4, 0, 0, 5, 4, 2, 3, 3, 0, 2, 5, 2, 1, 0, 0, 5,\n",
       "       0, 5, 0, 0, 0, 5, 5, 2, 5, 5, 0, 5, 2, 2, 0, 1, 0, 2, 5, 2, 5, 0,\n",
       "       0, 5, 5, 4, 0, 0, 2, 1, 0, 5, 0, 0, 2, 2, 5, 4, 0, 5, 1, 5, 4, 2,\n",
       "       2, 2, 0, 2, 0, 1, 5, 4, 1, 1, 0, 4, 1, 1, 3, 4, 0, 0, 6, 0, 0, 4]), metrics={'test_loss': 1.2595199346542358, 'test_f1': 0.7595851570602402, 'test_runtime': 0.5861, 'test_samples_per_second': 187.669, 'test_steps_per_second': 23.885})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8090909090909091, 0.7595851570602402)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "micro_f1 = f1_score(df_test['labels'],preds[0].argmax(axis = 1), average='micro')\n",
    "macro_f1 = f1_score(df_test['labels'],preds[0].argmax(axis = 1), average='macro')\n",
    "\n",
    "micro_f1, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 –∫–ª–∞—Å—Å–æ–≤\n",
    "# pr_txt f1 = 0.79\n",
    "# pr_txt_cleaned f1 = 0.927\n",
    "# sentences_cleaned f1 = 0.852\n",
    "\n",
    "# 17 –∫–ª–∞—Å—Å–æ–≤\n",
    "# pr_txt_cleaned f1 = 0.457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: lime in /usr/local/lib/python3.10/dist-packages (0.2.0.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime) (3.8.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime) (1.26.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime) (1.12.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lime) (4.66.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime) (1.2.0)\n",
      "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.22.0)\n",
      "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (3.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (10.2.0)\n",
      "Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.34.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2024.2.12)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (23.2)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (0.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (3.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14351/2333811124.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probas = F.softmax(tensor_logits).detach().numpy()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lime\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "class_names = ['AAA','AA', 'A', 'BBB', 'BB', 'B', 'C']\n",
    "\n",
    "def predictor(texts):\n",
    "    #dataset_tokenized = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "    dataset_tokenized = Dataset.from_dict(dict(tokenizer(texts, return_tensors=\"pt\", padding=True)))\n",
    "    #model.to('cpu')\n",
    "    #outputs = model(dataset_tokenized)\n",
    "    outputs = trainer.predict(dataset_tokenized)\n",
    "    tensor_logits = torch.tensor(outputs[0])\n",
    "    probas = F.softmax(tensor_logits).detach().numpy()\n",
    "    return probas\n",
    "\n",
    "# AA\n",
    "text = '¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø—Ä–∏—Å–≤–æ–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –ê–û ¬´–ü—Ä–æ—Å–≤–µ—â–µ–Ω–∏–µ¬ª –Ω–∞ —É—Ä–æ–≤–Ω–µ <rating>  –ú–æ—Å–∫–≤–∞, 27 –¥–µ–∫–∞–±—Ä—è 2022 –≥.  –†–µ–π—Ç–∏–Ω–≥–æ–≤–æ–µ –∞–≥–µ–Ω—Ç—Å—Ç–≤–æ ¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø—Ä–∏—Å–≤–æ–∏–ª–æ  —Ä–µ–π—Ç–∏–Ω–≥ –∫—Ä–µ–¥–∏—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏  –Ω–µ—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –∫–æ–º–ø–∞–Ω–∏–∏   –ê–û ¬´–ü—Ä–æ—Å–≤–µ—â–µ–Ω–∏–µ¬ª   –Ω–∞ —É—Ä–æ–≤–Ω–µ <rating>. –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ —Ä–µ–π—Ç–∏–Ω–≥—É - —Å—Ç–∞–±–∏–ª—å–Ω—ã–π. \") ¬´–ü—Ä–æ—Å–≤–µ—â–µ–Ω–∏–µ¬ª  ‚Äî –≤–µ–¥—É—â–∞—è —Ä–æ—Å—Å–∏–π—Å–∫–∞—è –∫–æ–º–ø–∞–Ω–∏—è, —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä—É—é—â–∞—è –≤ —Å–µ–≥–º–µ–Ω—Ç–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏  –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—â–∞—è –ø—Ä–æ–¥—É–∫—Ç—ã –∏ —É—Å–ª—É–≥–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –ø–µ—á–∞—Ç–Ω–æ–≥–æ –∏ —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞,  –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –æ–±—É—á–µ–Ω–∏—è –¥–µ—Ç–µ–π –∏ –≤–∑—Ä–æ—Å–ª—ã—Ö, –ø–æ—Å—Ç–∞–≤–æ–∫ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è –∏  –ø—Ä–∏–∫–ª–∞–¥–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤ —Ä–∞–º–∫–∞—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤ –≤ —Å—Ñ–µ—Ä–µ  –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è. –ö–æ–º–ø–∞–Ω–∏—è –≤—Ö–æ–¥–∏—Ç –≤ –ø–µ—Ä–µ—á–µ–Ω—å —Å–∏—Å—Ç–µ–º–æ–æ–±—Ä–∞–∑—É—é—â–∏—Ö –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–π –†–æ—Å—Å–∏–∏ –∏  –∞–∫—Ç–∏–≤–Ω–æ –≤–æ–≤–ª–µ—á–µ–Ω–∞ –≤ –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏ —Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã.  –ê–≥–µ–Ω—Ç—Å—Ç–≤–æ  –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä—ã–Ω–æ—á–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –∫–ª—é—á–µ–≤–æ–º –¥–ª—è –Ω–µ–µ  –∏–∑–¥–∞—Ç–µ–ª—å—Å–∫–æ–º —Ä—ã–Ω–∫–µ, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–∏–º –±–æ–ª—å—à—É—é —á–∞—Å—Ç—å –≤—ã—Ä—É—á–∫–∏ –∏ EBITDA. –ö–æ–º–ø–∞–Ω–∏—è –∑–∞–Ω–∏–º–∞–µ—Ç –ø–µ—Ä–≤–æ–µ –º–µ—Å—Ç–æ  –∫–∞–∫ –Ω–∞ —Ä—ã–Ω–∫–µ —É—á–µ–±–Ω–æ-–º–µ—Ç–æ–¥–∏—á–µ—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã (–£–ú–õ), —Ç–∞–∫ –∏ –ø–æ –æ–±—â–µ–º—É —Ç–∏—Ä–∞–∂—É  –Ω–µ–ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏—Ö –∏–∑–¥–∞–Ω–∏–π —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤ –†–æ—Å—Å–∏–∏. ¬´–ü—Ä–æ—Å–≤–µ—â–µ–Ω–∏–µ¬ª –∞–∫—Ç–∏–≤–Ω–æ  —Ä–∞–∑–≤–∏–≤–∞–µ—Ç —Ü–∏—Ñ—Ä–æ–≤—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã, —á—Ç–æ –º–æ–∂–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É—Å–∏–ª–∏—Ç—å –µ—ë —Ä—ã–Ω–æ—á–Ω—ã–µ  –ø–æ–∑–∏—Ü–∏–∏ –Ω–∞ —Ä—ã–Ω–∫–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∑–∞ —Å—á—ë—Ç —Å–∏–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ —Å –∏–∑–¥–∞—Ç–µ–ª—å—Å–∫–∏–º  –±–∏–∑–Ω–µ—Å–æ–º. –î–æ–ª—è —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –≤ –≤—ã—Ä—É—á–∫–µ –ø–æ–∫–∞ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º  —Å–µ–≥–º–µ–Ω—Ç —Ä–∞—Å—Ç—ë—Ç –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—ã–º–∏ —Ç–µ–º–ø–∞–º–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –±–∏–∑–Ω–µ—Å–∞–º–∏. –ù–∞  —Ñ–∞–∫—Ç–æ—Ä —Ä—ã–Ω–æ—á–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π, –ø–æ –º–Ω–µ–Ω–∏—é –∞–≥–µ–Ω—Ç—Å—Ç–≤–∞, –æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è  –≤—ã—Ä—É—á–∫–∏ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–µ –∏–∑–¥–∞—Ç–µ–ª—å—Å–∫–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Å—Ñ–µ—Ä–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π  –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –±–æ–ª–µ–µ 70% –≤—ã—Ä—É—á–∫–∏.  –í  —Ä–∞–º–∫–∞—Ö –∏–∑–¥–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –±–∏–∑–Ω–µ—Å–∞ ¬´–ü—Ä–æ—Å–≤–µ—â–µ–Ω–∏–µ¬ª –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é  —É—á–µ–±–Ω–æ-–º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å–æ–±–∏–π –∏ –∏—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏. –ö–æ–º–ø–∞–Ω–∏—è —É–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏  —É—á–µ–±–Ω—ã—Ö –ø–æ—Å–æ–±–∏–π, –≤–ª–∞–¥–µ–µ—Ç –∞–≤—Ç–æ—Ä—Å–∫–∏–º–∏ –ø—Ä–∞–≤–∞–º–∏ –Ω–∞ –Ω–∏—Ö, –æ—Ä–≥–∞–Ω–∏–∑—É–µ—Ç –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ  –≤ –ø–∞—Ä—Ç–Ω—ë—Ä—Å–∫–∏—Ö —Ç–∏–ø–æ–≥—Ä–∞—Ñ–∏—è—Ö, –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ—Å—Ç–∞–≤–∫—É —É—á–µ–±–Ω—ã—Ö –∏–∑–¥–∞–Ω–∏–π –≤ —Ä–∞–º–∫–∞—Ö  –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ —Å –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏. –ë–æ–ª–µ–µ 40% –ø–æ—Å—Ç–∞–≤–æ–∫  –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤. –ë–∏–∑–Ω–µ—Å –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –≤—ã—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–∏  –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –ø–∞—Ä—Ç–Ω—ë—Ä—Å–∫–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π —Å –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞–º–∏ –∏ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è–º–∏, –ø–æ—ç—Ç–æ–º—É —É  –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–µ—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–ª–∏ —Å–∫–ª–∞–¥—Å–∫–∏—Ö –ø–æ–º–µ—â–µ–Ω–∏–π.   ¬´–ü—Ä–æ—Å–≤–µ—â–µ–Ω–∏–µ¬ª  —Ç–∞–∫–∂–µ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è –æ—Å–Ω–∞—â–µ–Ω–∏–µ–º —à–∫–æ–ª —É—á–µ–±–Ω—ã–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ–º. –ë–ª–∞–≥–æ–¥–∞—Ä—è —à–∏—Ä–æ–∫–æ–π  –ø–∞—Ä—Ç–Ω—ë—Ä—Å–∫–æ–π —Å–µ—Ç–∏ –∫–æ–º–ø–∞–Ω–∏—è –º–æ–∂–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Å–Ω–∞—â–∞—Ç—å —à–∫–æ–ª—ã –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ  –∫–ª–∞—Å—Å—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ–º, –ø–æ—Å—Ç–∞–≤–ª—è—Ç—å –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ –≤ –±–æ–ª—å—à–æ–º –æ–±—ä—ë–º–µ –∏  –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–æ, —á—Ç–æ –≤—ã–≥–æ–¥–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç –µ—ë –æ—Ç –±–æ–ª–µ–µ –º–µ–ª–∫–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.  –†—ã–Ω–æ–∫ –æ—Å–Ω–∞—â–µ–Ω–∏—è –∏ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è —à–∫–æ–ª —Å–∏–ª—å–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –∏ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –±–æ–ª—å—à–æ–≥–æ  –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–µ–ª–∫–∏—Ö –∏–≥—Ä–æ–∫–æ–≤, —á—Ç–æ –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –±–æ–ª–µ–µ –∞–∫—Ç–∏–≤–Ω—É—é –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—é.  –ë–∞—Ä—å–µ—Ä—ã  –¥–ª—è –≤—Ö–æ–¥–∞ –≤ –æ—Ç—Ä–∞—Å–ª—å –æ—Ü–µ–Ω–µ–Ω—ã –Ω–∞ —É–º–µ—Ä–µ–Ω–Ω–æ–º —É—Ä–æ–≤–Ω–µ, –æ–¥–Ω–∞–∫–æ –Ω–æ–≤—ã–º –∏–≥—Ä–æ–∫–∞–º —Ä—ã–Ω–∫–∞  —Å–ª–æ–∂–Ω–æ –∫–æ–Ω–∫—É—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ –º–∞—Å—à—Ç–∞–±—É –∏ —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–µ —Å ¬´–ü—Ä–æ—Å–≤–µ—â–µ–Ω–∏–µ–º¬ª, —Ç—Ä–µ–±—É—é—â–∏–º  –±–æ–ª—å—à–∏—Ö –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π –≤ —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏ —É—Å–ª—É–≥ –¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è. –†—ã–Ω–æ–∫  –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –≤–Ω–µ—à–Ω–∏–º —à–æ–∫–∞–º. –ü–æ–¥–¥–µ—Ä–∂–∫—É –æ—Ü–µ–Ω–∫–µ  —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ –≤–Ω–µ—à–Ω–∏–º —à–æ–∫–∞–º'\n",
    "# BB\n",
    "#text = '¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª –∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –∫–æ–º–ø–∞–Ω–∏–∏ –û–û–û ¬´C–ï–õ–õ-–°–µ—Ä–≤–∏—Å¬ª –Ω–∞ —É—Ä–æ–≤–Ω–µ <rating>   –ú–æ—Å–∫–≤–∞, 17 —Ñ–µ–≤—Ä–∞–ª—è 2023 –≥.  –†–µ–π—Ç–∏–Ω–≥–æ–≤–æ–µ –∞–≥–µ–Ω—Ç—Å—Ç–≤–æ ¬´–≠–∫—Å–ø–µ—Ä—Ç –†–ê¬ª –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–æ  —Ä–µ–π—Ç–∏–Ω–≥ –∫—Ä–µ–¥–∏—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏  –Ω–µ—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –∫–æ–º–ø–∞–Ω–∏–∏   –û–û–û ¬´–°–ï–õ–õ-–°–µ—Ä–≤–∏—Å¬ª   –Ω–∞ —É—Ä–æ–≤–Ω–µ <rating>. –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ —Ä–µ–π—Ç–∏–Ω–≥—É - —Å—Ç–∞–±–∏–ª—å–Ω—ã–π.  \") –û–û–û  ¬´–°–ï–õ–õ-–°–µ—Ä–≤–∏—Å¬ª (–¥–∞–ª–µ–µ ‚Äì –∫–æ–º–ø–∞–Ω–∏—è) —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –Ω–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏  –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ –∫–æ–Ω–¥–∏—Ç–µ—Ä—Å–∫–æ–π –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏, –∞ —Ç–∞–∫–∂–µ –ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç  –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–π –º–æ–ª–æ—á–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞, –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ –Ω–∞–ø–∏—Ç–∫–æ–≤ –∏  –∫–æ–º–ø–∞–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç–∞ HoReCa. –ö–æ–º–ø–∞–Ω–∏—è —è–≤–ª—è–µ—Ç—Å—è –ø–æ—Å—Ç–∞–≤—â–∏–∫–æ–º –∫–∞–∫–∞–æ-–ø—Ä–æ–¥—É–∫—Ç–æ–≤,  –ø–∏—â–µ–≤–æ–π —Ö–∏–º–∏–∏, –∞—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä–æ–≤, –∞–≥–∞—Ä-–∞–≥–∞—Ä–æ–≤ –∏ –ø–µ–∫—Ç–∏–Ω–æ–≤, –∑–∞–∫–≤–∞—Å–æ—á–Ω—ã—Ö –∫—É–ª—å—Ç—É—Ä,  –∫—Ä–∞—Å–∏—Ç–µ–ª–µ–π –∏ –∫–∞—Ä–∞–º–µ–ª—å–Ω–æ–≥–æ –∫–æ–ª–µ—Ä–∞, –∑–∞–≥—É—Å—Ç–∏—Ç–µ–ª–µ–π, –Ω–∞–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π, –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω–æ–≥–æ  –ø—é—Ä–µ –∏ –æ–¥–Ω–æ—Ä–∞–∑–æ–≤–æ–π –ø–æ—Å—É–¥—ã –∏ —É–ø–∞–∫–æ–≤–∫–∏ Huhtamaki.   –ë–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ñ–∏–ª—å  –∫–æ–º–ø–∞–Ω–∏–∏ –æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ —É—Ä–æ–≤–µ–Ω—å —Ä–µ–π—Ç–∏–Ω–≥–∞. –ü–æ–¥–≤–µ—Ä–∂–µ–Ω–Ω–æ—Å—Ç—å  –∫–æ–º–ø–∞–Ω–∏–∏ –≤–Ω–µ—à–Ω–∏–º —à–æ–∫–∞–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç—Å—è –∫–∞–∫ –≤—ã—Å–æ–∫–∞—è –≤–≤–∏–¥—É –Ω–∞–ª–∏—á–∏—è —Å–∏–ª—å–Ω–æ–π  –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ ‚Äì –æ–∫–æ–ª–æ 80% –≤—Å–µ—Ö –ø–æ—Å—Ç–∞–≤–æ–∫. –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å  –∏–º–ø–æ—Ä—Ç–∞ –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ –ò–Ω–¥–æ–Ω–µ–∑–∏—é (47% –≤—Å–µ–≥–æ –∏–º–ø–æ—Ä—Ç–∞ –∑–∞ 9 –º–µ—Å—è—Ü–µ–≤ 2022 –≥–æ–¥–∞),  –ö–∏—Ç–∞–π (28%) –∏ –í—å–µ—Ç–Ω–∞–º (7%). –û—Ç—Ä–∞—Å–ª—å –ø–æ–¥–≤–µ—Ä–∂–µ–Ω–∞ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏–º —Ä–∏—Å–∫–∞–º –Ω–∞ —Ñ–æ–Ω–µ  –∑–∞–¥–µ—Ä–∂–µ–∫ –º–æ—Ä—Å–∫–∏—Ö –ø–æ—Å—Ç–∞–≤–æ–∫, —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Å—Ä–æ–∫–æ–≤ —Ä–∞–∑–≥—Ä—É–∑–∫–∏ –≤ –ø–æ—Ä—Ç–∞—Ö,  –ø—Ä–æ–¥–æ–ª–∂–∞—é—â–µ–≥–æ—Å—è —Ä–æ—Å—Ç–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ —Ñ—Ä–∞—Ö—Ç–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏  –∂–µ–ª–µ–∑–Ω–æ–π –¥–æ—Ä–æ–≥–∏ –≤ –†–æ—Å—Å–∏–∏, –ø—Ä–∏ —ç—Ç–æ–º –≤ —É—Å–ª–æ–≤–∏—è—Ö –≥–µ–æ–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ —à–æ–∫–∞ 2022 –≥–æ–¥–∞  –∫–æ–º–ø–∞–Ω–∏–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–æ–≥–æ–≤–æ—Ä–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è, —Å—Ä–æ–∫–∏ –∏  —É—Å–ª–æ–≤–∏—è –ø–æ—Å—Ç–∞–≤–æ–∫ —Å –∫–ª—é—á–µ–≤—ã–º–∏ –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞–º–∏ –∏ –Ω–∞—Ä–∞—Å—Ç–∏—Ç—å –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É. –ê–≥–µ–Ω—Ç—Å—Ç–≤–æ  –æ—Ç–º–µ—á–∞–µ—Ç –Ω–∞–ª–∏—á–∏–µ —É –∫–æ–º–ø–∞–Ω–∏–∏ –∑–∞–∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–º–æ–≤ –ø–æ—Å—Ç–∞–≤–æ–∫ –Ω–∞ 2023 –≥–æ–¥ –ø–æ  –æ—Å–Ω–æ–≤–Ω—ã–º —Ç–æ–≤–∞—Ä–Ω—ã–º –ø–æ–∑–∏—Ü–∏—è–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —Å—Ä–æ–∫–∏ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è  –ø—Ä–æ–¥—É–∫—Ü–∏–∏ –¥–ª—è –ø–µ—Ä–µ–ø—Ä–æ–¥–∞–∂–∏. –°–∏—Å—Ç–µ–º–∞ —Ä–∞—Å—á–µ—Ç–æ–≤ —Å –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞–º–∏ –ø—Ä–µ–¥—É—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç  –ø—Ä–µ–¥–æ–ø–ª–∞—Ç—É, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –æ–∫–æ–ª–æ 70% –≤—ã—Ä—É—á–∫–∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –∑–∞ —Å—á–µ—Ç –∑–∞–∫–∞–∑–æ–≤ —Å  –æ—Ç—Å—Ä–æ—á–∫–æ–π –ø–ª–∞—Ç–µ–∂–∞. –ó–∞–∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤–∞–Ω–Ω—ã–µ —Å –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞–º–∏ –æ–±—ä–µ–º—ã –∏–º–ø–æ—Ä—Ç–Ω—ã—Ö –∑–∞–∫—É–ø–æ–∫  –æ—Å–Ω–æ–≤—ã–≤–∞—é—Ç—Å—è –Ω–∞ —Ä–µ—Ç—Ä–æ—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –æ–±—ä–µ–º–∞—Ö –æ—Ç–≥—Ä—É–∑–æ–∫ —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º —Ä–æ—Å—Ç–∞, –Ω–æ –ø—Ä–∏  —ç—Ç–æ–º –Ω–µ –æ–±–µ—Å–ø–µ—á–µ–Ω—ã –¥–æ–≥–æ–≤–æ—Ä–Ω—ã–º–∏ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏ —Å –±—É–¥—É—â–∏–º–∏ –ø–æ–∫—É–ø–∞—Ç–µ–ª—è–º–∏. –†—ã–Ω–æ–∫  —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç—Å—è –Ω–∏–∑–∫–∏–º–∏ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–º–∏ –±–∞—Ä—å–µ—Ä–∞–º–∏ –¥–ª—è –≤—Ö–æ–¥–∞.  –ê–≥–µ–Ω—Ç—Å—Ç–≤–æ  –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä—ã–Ω–æ—á–Ω—ã–µ –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–∞ —Å—Ä–µ–¥–Ω–µ–º —É—Ä–æ–≤–Ω–µ. –í–≤–∏–¥—É  –Ω–µ–±–æ–ª—å—à–∏—Ö —Ç–µ–∫—É—â–∏—Ö –º–∞—Å—à—Ç–∞–±–æ–≤ –±–∏–∑–Ω–µ—Å–∞ –ø–æ–∑–∏—Ü–∏–∏ –Ω–∞ —Ä—ã–Ω–∫–∞—Ö —Å–±—ã—Ç–∞ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è  —É–º–µ—Ä–µ–Ω–Ω–æ-–Ω–µ–≥–∞—Ç–∏–≤–Ω–æ, –¥–æ–ª—è –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –∫–∞–∫–∞–æ-–ø–æ—Ä–æ—à–∫—É –∏ –∞–≥–∞—Ä-–∞–≥–∞—Ä–∞–º –Ω–∞ —Ä—ã–Ω–∫–µ  –†–æ—Å—Å–∏–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ –ø–æ 6%, –ø–æ –æ—Å—Ç–∞–ª—å–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Ç–æ–≤–∞—Ä–∞–º –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç 3%.  –ê—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç–Ω—ã–π –ø–æ—Ä—Ç—Ñ–µ–ª—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç—Å—è —É–º–µ—Ä–µ–Ω–Ω–æ-–≤—ã—Å–æ–∫–æ–π —Å—Ç–µ–ø–µ–Ω—å—é  –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –¢–∞–∫ –Ω–∞ –∫—Ä—É–ø–Ω–µ–π—à—É—é –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–Ω—É—é –ø–æ–∑–∏—Ü–∏—é ‚Äì –∫–∞–∫–∞–æ-–ø–æ—Ä–æ—à–æ–∫ ‚Äì –ø—Ä–∏—à–ª–æ—Å—å  40% –æ—Ç –æ–±—â–∏—Ö –ø—Ä–æ–¥–∞–∂, –ª–∏–º–æ–Ω–Ω–æ–π –∫–∏—Å–ª–æ—Ç—ã ‚Äì 18%, –¥–æ–ª–∏ –¥—Ä—É–≥–∏—Ö –ø–æ–∑–∏—Ü–∏–π –Ω–µ –ø—Ä–µ–≤—ã—à–∞—é—Ç 6%.'\n",
    "#print(tokenizer(text, return_tensors='pt', padding=True))\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "exp = explainer.explain_instance(text, predictor, num_features = 2048, num_samples=1000, top_labels = 7)\n",
    "\n",
    "exp.show_in_notebook(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d3007d64cea4a50bcf32012fee8d17d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da4fac23ccbf4c289bc27ede999c2be9",
      "max": 960,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e2b1dd5ac52f4f1795f3e39bc66b998b",
      "value": 960
     }
    },
    "28f57120d6d84cc2a421a3815fe3d6fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abdebdbc5bad41088115f35fb2f1d5d6",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_673cb2e6bd2c4676ba47d1c072f3675c",
      "value": "‚Äá240/240‚Äá[00:10&lt;00:00,‚Äá23.54‚Äáexamples/s]"
     }
    },
    "2aaeda6ccd094cc3996cce35823f695e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2dbe750eb36f44afb572760c6361217e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "37e17a11a1b74296a3ac0af683721623": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4fa69ef96038425196e202e85f65c419",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_2aaeda6ccd094cc3996cce35823f695e",
      "value": "‚Äá1200/0‚Äá[00:00&lt;00:00,‚Äá1996.44‚Äáexamples/s]"
     }
    },
    "3dcbfc7b131c494facccd1467d5fde0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4944065d9f1544a39718986e2e9fe33d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4bb90ce8f6794cdc80008b0b348332a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a05b2fe8e9b54d92a2dcbde4d727e66a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_63a7e6d20dd04a21bdd73f0c5045bd82",
      "value": "Map:‚Äá100%"
     }
    },
    "4fa69ef96038425196e202e85f65c419": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5bbe8d24867a4cef9c899bf9b4667665": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4bb90ce8f6794cdc80008b0b348332a3",
       "IPY_MODEL_8189b7338ace4f379bdbe77dc48b8e7e",
       "IPY_MODEL_28f57120d6d84cc2a421a3815fe3d6fe"
      ],
      "layout": "IPY_MODEL_71952a9a8c944a1783d38195eae1550b"
     }
    },
    "63a7e6d20dd04a21bdd73f0c5045bd82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6605ce14652346a182a31dbc69d6db9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "673cb2e6bd2c4676ba47d1c072f3675c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "71952a9a8c944a1783d38195eae1550b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75254c71403847e49dfed51646a30bdf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b4abf3093c441b98e280cd14256fdd0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80207e4e50e2459984cf2beab036f66b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2dbe750eb36f44afb572760c6361217e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3dcbfc7b131c494facccd1467d5fde0f",
      "value": 1
     }
    },
    "8189b7338ace4f379bdbe77dc48b8e7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b4abf3093c441b98e280cd14256fdd0",
      "max": 240,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4944065d9f1544a39718986e2e9fe33d",
      "value": 240
     }
    },
    "892587c0220b4b739ac6ac653483138f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8be92b5a3a364009b6af55286c91560b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae212bf556394b92aefa01a0c5274bc7",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_acb512b19fee440880d3ec6864be4669",
      "value": "Generating‚Äátrain‚Äásplit:‚Äá"
     }
    },
    "8dcd632c9cab4753bdfc157e692e7fca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a05b2fe8e9b54d92a2dcbde4d727e66a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3ff77b4fb7c45e7af060e8871e9c1c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7d656e93e0c4fbda2ea8226996f7e22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abdebdbc5bad41088115f35fb2f1d5d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acb512b19fee440880d3ec6864be4669": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae212bf556394b92aefa01a0c5274bc7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afdfc15ad67b4088821d3b2d650b88ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7d656e93e0c4fbda2ea8226996f7e22",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6605ce14652346a182a31dbc69d6db9e",
      "value": "‚Äá960/960‚Äá[01:01&lt;00:00,‚Äá15.59‚Äáexamples/s]"
     }
    },
    "b07850732b5e43b2971e8e22411ac3f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8be92b5a3a364009b6af55286c91560b",
       "IPY_MODEL_80207e4e50e2459984cf2beab036f66b",
       "IPY_MODEL_37e17a11a1b74296a3ac0af683721623"
      ],
      "layout": "IPY_MODEL_892587c0220b4b739ac6ac653483138f"
     }
    },
    "b75811eaa63f40f0bb9a6e908db56001": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cf10dc85f4054f42afc998c4610cc88f",
       "IPY_MODEL_0d3007d64cea4a50bcf32012fee8d17d",
       "IPY_MODEL_afdfc15ad67b4088821d3b2d650b88ce"
      ],
      "layout": "IPY_MODEL_8dcd632c9cab4753bdfc157e692e7fca"
     }
    },
    "cf10dc85f4054f42afc998c4610cc88f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75254c71403847e49dfed51646a30bdf",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a3ff77b4fb7c45e7af060e8871e9c1c5",
      "value": "Map:‚Äá100%"
     }
    },
    "da4fac23ccbf4c289bc27ede999c2be9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2b1dd5ac52f4f1795f3e39bc66b998b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
